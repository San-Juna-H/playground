{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc4125f4caf42478775af4260c5340e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFacePipeline\n",
    "Among transformers, the Pipeline is the most versatile tool in the Hugging Face toolbox. LangChain being designed primarily to address RAG and Agent use cases, the scope of the pipeline here is reduced to the following text-centric tasks: “text-generation\", “text2text-generation\", “summarization”, “translation”.\n",
    "\n",
    "Models can be loaded directly with the from_model_id method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "llm.invoke(\"Hugging Face is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this class, the model will be loaded in cache and use your computer’s hardware; thus, you may be limited by the available resources on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFaceEndpoint\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a popular open-source library for natural language processing (NLP) tasks. One of its key features is its support for transfer learning, which allows developers to fine-tune pre-trained language models for specific tasks. However, the process of fine-tuning a model can be time-consuming and requires a significant amount of computational resources.\\n\\nTo address these challenges, Hugging Face has introduced a new feature called \"Hugging Face Cloud,\" which provides a cloud-based platform for fine-tuning pre-trained models. With'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    ")\n",
    "llm.invoke(\"Hugging Face is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, this class uses the InferenceClient to be able to serve a wide variety of use-case, serverless API to deployed TGI instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatHuggingFace\n",
    "\n",
    "Every model has its own special tokens with which it works best. Without those tokens added to your prompt, your model will greatly underperform.\n",
    "\n",
    "When going from a list of messages to a completion prompt, there is an attribute that exists in most LLM tokenizers called chat_template.\n",
    "\n",
    "To learn more about chat_template in the different models, visit this space I made!\n",
    "\n",
    "This class is wrapper around the other LLMs. It takes as input a list of messages an then creates the correct completion prompt by using the tokenizer.apply_chat_template method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    ")\n",
    "llm_engine_hf = ChatHuggingFace(llm=llm)\n",
    "llm_engine_hf.invoke(\"Hugging Face is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is equivalent to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# with mistralai/Mistral-7B-Instruct-v0.2\n",
    "llm.invoke(\"<s>[INST] Hugging Face is [/INST]\")\n",
    "\n",
    "# with meta-llama/Meta-Llama-3-8B-Instruct\n",
    "llm.invoke(\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Hugging Face is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
